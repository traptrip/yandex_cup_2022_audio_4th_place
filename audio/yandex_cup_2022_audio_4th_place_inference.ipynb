{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4806c0ca",
   "metadata": {},
   "source": [
    "# [Inference] YandexCup 2022 - ML Audio Content 4th place solution\n",
    "- **Training code** - https://github.com/traptrip/yandex_cup_2022_audio_4th_place\n",
    "- **Offline Diffusion** - https://github.com/fyang93/diffusion\n",
    "\n",
    "## Предсказание исполнителя трека по набору акустических признаков\n",
    "### Описание задачи\n",
    "\n",
    "На первый взгляд задача предсказания исполнителя трека выглядит странной, так как кажется, что эта информация изначально нам известна. Но при ближайшем рассмотрении, оказывается что не все так просто. Во-первых, есть задача разделения одноименных исполнителей. Когда к нам в каталог поступает новый релиз, то нам нужно как-то сопоставить исполнителя этого релиза с теми что уже есть в нашей базе и для одноименных исполнителей возникает неоднозначность. Во-вторых, и это менее очевидная часть, предсказывая исполнителей по аудио, мы неявным образом получаем модель которая выучивает похожесть исполнителей по звучанию и это также может быть полезным.\n",
    "\n",
    "### Формат входных данных\n",
    "\n",
    "По лицензионным соглашениям мы не можем выкладывать исходные аудио треки, поэтому в рамках данной задачи мы решили подготовить для каждого трека признаковое описание на основе аудио сигнала. Изначально выбирается случайный фрагмент трека из центральной его части (70 процентов трека) длительностью 60 секунд, если трек короче 60 секунд, то берется трек целиком. Далее, этот фрагмент разбивается на чанки размером около 1.5 секунд и шагом порядка 740 миллисекунд и затем для каждого такого чанка аудио сигнала вычисляется вектор чисел, описывающий этот чанк, размером 512, это своего рода эмбединг этого чанка. Таким образом для каждого трека мы получаем последовательность векторов или другими словами матрицу размером 512xT сохраненную в файл в виде numpy array. Во входных данных задачи есть следующие файлы:\n",
    "- train_features.tar.gz\n",
    "- train_features_sample.tar.gz\n",
    "- test_features.tar.gz\n",
    "- train_meta.tsv\n",
    "- train_sample_meta.tsv\n",
    "- test_meta.tsv\n",
    "- compute_score.py\n",
    "- naive_baseline.py\n",
    "- nn_baseline.py\n",
    "\n",
    "Первые два файла train_features.tar.gz и test_features.tar.gz это архивы с файлами, в которых хранятся признаковые описания треков обучающего и тестового подмножества соответственно.\n",
    "Файл train_meta.tsv содержит отображение id треков в id исполнителей и дополнительно ссылку на относительный путь к файлу с признаковым описанием трека в архиве. Файл test_meta.tsv имеет аналогичный формат, за той лишь разницей что в нем нет id исполнителей треков. Мы отбирали треки таким образом, чтобы множества исполнителей в обучающем и тестовом подмножествах не пересекались.\n",
    "\n",
    "Файл compute_score.py поможет вам посчитать метрику для вашего решения. Для простоты выделения валидационного подмножества треков, на котором можно оценивать метрику локально, мы разбили обучающее множество треков на 10 поддиректорий, внутри каждой из них исполнители треков также не пересекаются.\n",
    "\n",
    "Файл naive_baseline.py содержит пример наивного решения, показывает как загружать из файла признаковые описания треков и как формировать файл с решением.\n",
    "\n",
    "Файл nn_baseline.py содержит пример простейшего решения на основе нейросетей с использованием фреймворка pytorch\n",
    "\n",
    "Кроме того мы добавили файлы train_features_sample.tar.gz и train_sample_meta.tsv которые содержат сэмпл данных для обучения\n",
    "\n",
    "Все файлы можно скачать по ссылке: https://disk.yandex.ru/d/xKv1B88WtLZnPw\n",
    "\n",
    "Альтернативно можно скачать данные (без скриптов) по ссылке https://storage.yandexcloud.net/audioml-contest22/dataset.tar.gz Сэмпл данных для обучения доступен по ссылке https://storage.yandexcloud.net/audioml-contest22/train_features_sample.tar.gz\n",
    "\n",
    "### Формат решения\n",
    "\n",
    "Так как исполнители в обучающем и тестовом подмножествах разные, мы не можем подходить к решению этой задачи в лоб как к задаче классификации. Поэтому, задача заключается в том чтобы для каждого трека из тестового множества, по аудио признакам трека, построить отранжированный список остальных треков из тестового множества (исключая исходных трек-запрос для которого мы строим текущий список), такой, что чем выше трек в этом списке тем более вероятно что он принадлежит тому же исполнителю что и исходный трек-запрос. Для каждого трека из тестового множества нужно вывести одну строку в итоговый файл с решением. Формат строки query_trackid <tab> trackid1 <space> trackid2 … trackid100\n",
    "\n",
    "Качество решения мы будем мерить с помощью метрики nDCG@100 (Normalized Discounted Cumulative Gain at K, K=100)\n",
    "\n",
    "Ссылка на википедию: https://en.wikipedia.org/wiki/Discounted_cumulative_gain#Normalized_DCG\n",
    "\n",
    "Во время соревнования в лидерборде будет отображаться максимальный public score из всех ваших валидных посылок. После завершения соревнования, лидерборд будет переранжирован согласно private score вашей последней валидной посылки. Кроме этого в лидерборде будет отображаться public score посчитанный также по вашему последнему валидному решению. Это означает что он может не совпадать (в частности быть ниже) с вашим максимальным public score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c44c6e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import copy\n",
    "import random\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import joblib\n",
    "import torch\n",
    "import faiss\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import scipy.sparse as sparse\n",
    "import scipy.sparse.linalg as linalg\n",
    "from joblib import Parallel, delayed\n",
    "from sklearn import preprocessing\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5aa872ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cfg:\n",
    "    SEED = 42\n",
    "    CROP_SIZE = 81\n",
    "    N_FOLDS = 10\n",
    "    DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "\n",
    "    TEST_DATA_DIR = \"./data/audio/test_features\"\n",
    "    TEST_CSV_PATH = \"./data/audio/test_meta.tsv\"\n",
    "    \n",
    "    TRAIN_DATA_DIR = \"./data/audio/train_features\"\n",
    "    TRAIN_CSV_PATH = \"./data/audio/train_meta_with_stages.tsv\"\n",
    "\n",
    "    SUBMISSION_PATH = \"submission.txt\"\n",
    "    CHECKPOINT_PATH = \"./experiments/audio/folds_weights_f10\"\n",
    "    BATCH_SIZE = 512\n",
    "\n",
    "# Seed\n",
    "seed = cfg.SEED\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.cuda.manual_seed_all(seed)\n",
    "torch.backends.cudnn.benchmark = False\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "376a2032",
   "metadata": {},
   "source": [
    "# Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "00a48cb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_ranked_list_diffusion(embeds, top_size=100):\n",
    "    k = top_size + 1\n",
    "    targets, embeddings = embeds\n",
    "    ranks = offline_diffusion_search(embeddings, embeddings, None, truncation_size=k, kd=k)\n",
    "    ranks = targets[ranks]\n",
    "\n",
    "    ranked_list = dict()\n",
    "    cnt = []\n",
    "    for i, track_id in enumerate(targets):\n",
    "        candidates = list(filter(lambda x: x != track_id, ranks[i, 1:]))\n",
    "        ranked_list[track_id] = candidates[:100]\n",
    "        cnt.append(len(ranked_list[track_id]))\n",
    "    print(min(cnt), max(cnt), np.mean(cnt))\n",
    "\n",
    "    return ranked_list\n",
    "\n",
    "\n",
    "def inference(models, loader):\n",
    "    all_embeddings = []\n",
    "    all_track_ids = []\n",
    "\n",
    "    for data in tqdm(loader):\n",
    "        features = data[\"features\"].to(cfg.DEVICE)\n",
    "        track_ids = data[\"track_id\"].tolist()\n",
    "\n",
    "        embeddings = []\n",
    "        for model in models:\n",
    "            with torch.no_grad():\n",
    "                embeddings.append(model(features))\n",
    "        embeddings = torch.concat(embeddings, dim=-1)\n",
    "\n",
    "        all_embeddings.append(embeddings.cpu())\n",
    "        all_track_ids.extend(track_ids)\n",
    "\n",
    "    all_embeddings = torch.concat(all_embeddings, dim=0)\n",
    "    all_track_ids = np.array(all_track_ids)\n",
    "\n",
    "    return all_track_ids, all_embeddings\n",
    "\n",
    "\n",
    "def position_discounter(position):\n",
    "    return 1.0 / np.log2(position + 1)\n",
    "\n",
    "\n",
    "def get_ideal_dcg(relevant_items_count, top_size):\n",
    "    dcg = 0.0\n",
    "    for result_indx in range(min(top_size, relevant_items_count)):\n",
    "        position = result_indx + 1\n",
    "        dcg += position_discounter(position)\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def compute_dcg(query_trackid, ranked_list, track2artist_map, top_size):\n",
    "    query_artistid = track2artist_map[query_trackid]\n",
    "    dcg = 0.0\n",
    "    for result_indx, result_trackid in enumerate(ranked_list[:top_size]):\n",
    "        assert result_trackid != query_trackid\n",
    "        position = result_indx + 1\n",
    "        discounted_position = position_discounter(position)\n",
    "        result_artistid = track2artist_map[result_trackid]\n",
    "        if result_artistid == query_artistid:\n",
    "            dcg += discounted_position\n",
    "    return dcg\n",
    "\n",
    "\n",
    "def eval_submission(submission, gt_meta_info, top_size=100):\n",
    "    track2artist_map = gt_meta_info.set_index(\"trackid\")[\"artistid\"].to_dict()\n",
    "    artist2tracks_map = gt_meta_info.groupby(\"artistid\").agg(list)[\"trackid\"].to_dict()\n",
    "    ndcg_list = []\n",
    "    for query_trackid in tqdm(submission.keys()):\n",
    "        ranked_list = submission[query_trackid]\n",
    "        query_artistid = track2artist_map[query_trackid]\n",
    "        query_artist_tracks_count = len(artist2tracks_map[query_artistid])\n",
    "        ideal_dcg = get_ideal_dcg(query_artist_tracks_count - 1, top_size=top_size)\n",
    "        dcg = compute_dcg(\n",
    "            query_trackid, ranked_list, track2artist_map, top_size=top_size\n",
    "        )\n",
    "        try:\n",
    "            ndcg_list.append(dcg / ideal_dcg)\n",
    "        except ZeroDivisionError:\n",
    "            continue\n",
    "    return np.mean(ndcg_list)\n",
    "\n",
    "\n",
    "def save_submission(submission, submission_path):\n",
    "    with open(submission_path, \"w\") as f:\n",
    "        for query_trackid, result in submission.items():\n",
    "            f.write(\"{}\\t{}\\n\".format(query_trackid, \" \".join(map(str, result))))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd98d29",
   "metadata": {},
   "source": [
    "# Offline Diffusion Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2ce774a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseKNN(object):\n",
    "    \"\"\"KNN base class\"\"\"\n",
    "    def __init__(self, embeddings: np.ndarray, ids: Optional[np.ndarray], method):\n",
    "        if embeddings.dtype != np.float32:\n",
    "            embeddings = embeddings.astype(np.float32)\n",
    "        self.N = len(embeddings)\n",
    "        self.D = embeddings[0].shape[-1]\n",
    "        self.embeddings = embeddings if embeddings.flags['C_CONTIGUOUS'] \\\n",
    "                               else np.ascontiguousarray(embeddings)\n",
    "        self.labels = ids\n",
    "\n",
    "    def add(self, batch_size=10000):\n",
    "        \"\"\"Add data into index\"\"\"\n",
    "        if self.N <= batch_size:\n",
    "            self.index.add(self.embeddings)\n",
    "        else:\n",
    "            [self.index.add(self.embeddings[i:i+batch_size])\n",
    "                    for i in range(0, len(self.embeddings), batch_size)]\n",
    "\n",
    "    def search(self, queries, k=5):\n",
    "        \"\"\"Search\n",
    "        Args:\n",
    "            queries: query vectors\n",
    "            k: get top-k results\n",
    "        Returns:\n",
    "            sims: similarities of k-NN\n",
    "            ids: indexes of k-NN\n",
    "        \"\"\"\n",
    "        if not queries.flags['C_CONTIGUOUS']:\n",
    "            queries = np.ascontiguousarray(queries)\n",
    "        if queries.dtype != np.float32:\n",
    "            queries = queries.astype(np.float32)\n",
    "        sims, ids = self.index.search(queries, k)\n",
    "        ids = self.labels[ids] if self.labels is not None else ids\n",
    "        return sims, ids\n",
    "\n",
    "\n",
    "class KNN(BaseKNN):\n",
    "    \"\"\"KNN class\n",
    "    Args:\n",
    "        embeddings: feature vectors in database\n",
    "        ids: labels of feature vectors\n",
    "        method: distance metric\n",
    "    \"\"\"\n",
    "    def __init__(self, embeddings: np.ndarray, ids: np.ndarray, method):\n",
    "        super().__init__(embeddings, ids, method)\n",
    "        self.index = {\n",
    "            'cosine': faiss.IndexFlatIP,\n",
    "            'euclidean': faiss.IndexFlatL2,\n",
    "        }[method](self.D)\n",
    "        if os.environ.get('CUDA_VISIBLE_DEVICES'):\n",
    "            self.index = faiss.index_cpu_to_all_gpus(self.index)\n",
    "        self.labels = ids\n",
    "        self.add()\n",
    "\n",
    "\n",
    "class ANN(BaseKNN):\n",
    "    \"\"\"Approximate nearest neighbor search class\n",
    "    Args:\n",
    "        embeddings: feature vectors in database\n",
    "        ids: labels of feature vectors\n",
    "        method: distance metric\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self, embeddings: np.ndarray, ids: Optional[np.ndarray], method=\"cosine\", M=128, nbits=8, nlist=316, nprobe=64\n",
    "    ):\n",
    "        super().__init__(embeddings, ids, method)\n",
    "        self.labels = ids\n",
    "        self.quantizer = {\n",
    "            'cosine': faiss.IndexFlatIP,\n",
    "            'euclidean': faiss.IndexFlatL2\n",
    "        }[method](self.D)\n",
    "        self.index = faiss.IndexIVFPQ(self.quantizer, self.D, nlist, M, nbits)\n",
    "        samples = embeddings[np.random.permutation(np.arange(self.N))[:self.N // 5]]\n",
    "        self.index.train(samples)\n",
    "        self.add()\n",
    "        self.index.nprobe = nprobe\n",
    "\n",
    "\n",
    "trunc_ids = None\n",
    "trunc_init = None\n",
    "lap_alpha = None\n",
    "\n",
    "\n",
    "def get_offline_result(i):\n",
    "    ids = trunc_ids[i]\n",
    "    trunc_lap = lap_alpha[ids][:, ids]\n",
    "    scores, _ = linalg.cg(trunc_lap, trunc_init, tol=1e-6, maxiter=20)\n",
    "    return scores\n",
    "\n",
    "\n",
    "def cache(filename):\n",
    "    \"\"\"Decorator to cache results\"\"\"\n",
    "\n",
    "    def decorator(func):\n",
    "        def wrapper(*args, **kw):\n",
    "            self = args[0]\n",
    "            path = os.path.join(self.cache_dir, filename)\n",
    "            Path(self.cache_dir).mkdir(parents=True, exist_ok=True)\n",
    "            time0 = time.time()\n",
    "            if os.path.exists(path):\n",
    "                result = joblib.load(path)\n",
    "                cost = time.time() - time0\n",
    "                # print(\"[cache] loading {} costs {:.2f}s\".format(path, cost))\n",
    "                return result\n",
    "            result = func(*args, **kw)\n",
    "            cost = time.time() - time0\n",
    "            print(\"[cache] obtaining {} costs {:.2f}s\".format(path, cost))\n",
    "            joblib.dump(result, path)\n",
    "            return result\n",
    "\n",
    "        return wrapper\n",
    "\n",
    "    return decorator\n",
    "\n",
    "\n",
    "class Diffusion(object):\n",
    "    \"\"\"Diffusion class\"\"\"\n",
    "\n",
    "    def __init__(self, features, labels, cache_dir):\n",
    "        self.features = features\n",
    "        self.labels = np.array(labels)\n",
    "        self.N = len(self.features)\n",
    "        self.cache_dir = cache_dir\n",
    "        # use ANN for large datasets\n",
    "        self.use_ann = self.N >= 1_000_000\n",
    "        if self.use_ann:\n",
    "            print(\"ANN creating...\")\n",
    "            self.ann = ANN(self.features, None, method=\"cosine\")\n",
    "        self.knn = KNN(self.features, None, method=\"cosine\")\n",
    "\n",
    "    # @cache(\"offline.jbl\")\n",
    "    def get_offline_results(self, n_trunc, kd=50):\n",
    "        \"\"\"Get offline diffusion results for each gallery feature\"\"\"\n",
    "        print(\"[offline] starting offline diffusion\")\n",
    "        print(\"[offline] 1) prepare Laplacian and initial state\")\n",
    "        global trunc_ids, trunc_init, lap_alpha\n",
    "        trunc_ids = None\n",
    "        trunc_init = None\n",
    "        lap_alpha = None\n",
    "\n",
    "        if self.use_ann:\n",
    "            _, trunc_ids = self.ann.search(self.features, n_trunc)\n",
    "            sims, ids = self.knn.search(self.features, kd)\n",
    "            lap_alpha = self.get_laplacian(sims, ids)\n",
    "        else:\n",
    "            sims, ids = self.knn.search(self.features, n_trunc)\n",
    "            trunc_ids = ids\n",
    "            lap_alpha = self.get_laplacian(sims[:, :kd], ids[:, :kd])\n",
    "        trunc_init = np.zeros(n_trunc)\n",
    "        trunc_init[0] = 1\n",
    "        \n",
    "        print(\"[offline] 2) gallery-side diffusion\")\n",
    "        results = Parallel(n_jobs=-1, prefer=\"threads\")(\n",
    "            delayed(get_offline_result)(i)\n",
    "            for i in tqdm(range(self.N), desc=\"[offline] diffusion\")\n",
    "        )\n",
    "        all_scores = np.concatenate(results)\n",
    "\n",
    "        print(\"[offline] 3) merge offline results\")\n",
    "        rows = np.repeat(np.arange(self.N), n_trunc)\n",
    "\n",
    "        offline = sparse.csr_matrix(\n",
    "            (all_scores, (rows, trunc_ids.reshape(-1))),\n",
    "            shape=(self.N, self.N),\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        return offline\n",
    "\n",
    "    # @cache('laplacian.jbl')\n",
    "    def get_laplacian(self, sims, ids, alpha=0.99):\n",
    "        \"\"\"Get Laplacian_alpha matrix\"\"\"\n",
    "        affinity = self.get_affinity(sims, ids)\n",
    "        num = affinity.shape[0]\n",
    "        degrees = affinity @ np.ones(num) + 1e-12\n",
    "        # mat: degree matrix ^ (-1/2)\n",
    "        mat = sparse.dia_matrix(\n",
    "            (degrees ** (-0.5), [0]), shape=(num, num), dtype=np.float32\n",
    "        )\n",
    "        stochastic = mat @ affinity @ mat\n",
    "        sparse_eye = sparse.dia_matrix(\n",
    "            (np.ones(num), [0]), shape=(num, num), dtype=np.float32\n",
    "        )\n",
    "        lap_alpha = sparse_eye - alpha * stochastic\n",
    "        return lap_alpha\n",
    "\n",
    "    # @cache('affinity.jbl')\n",
    "    def get_affinity(self, sims, ids, gamma=3):\n",
    "        \"\"\"Create affinity matrix for the mutual kNN graph of the whole dataset\n",
    "        Args:\n",
    "            sims: similarities of kNN\n",
    "            ids: indexes of kNN\n",
    "        Returns:\n",
    "            affinity: affinity matrix\n",
    "        \"\"\"\n",
    "        num = sims.shape[0]\n",
    "        sims[sims < 0] = 0  # similarity should be non-negative\n",
    "        sims = sims**gamma\n",
    "        # vec_ids: feature vectors' ids\n",
    "        # mut_ids: mutual (reciprocal) nearest neighbors' ids\n",
    "        # mut_sims: similarites between feature vectors and their mutual nearest neighbors\n",
    "        vec_ids, mut_ids, mut_sims = [], [], []\n",
    "        for i in range(num):\n",
    "            # check reciprocity: i is in j's kNN and j is in i's kNN when i != j\n",
    "            ismutual = np.isin(ids[ids[i]], i).any(axis=1)\n",
    "            ismutual[0] = False\n",
    "            if ismutual.any():\n",
    "                vec_ids.append(i * np.ones(ismutual.sum(), dtype=int))\n",
    "                mut_ids.append(ids[i, ismutual])\n",
    "                mut_sims.append(sims[i, ismutual])\n",
    "        vec_ids, mut_ids, mut_sims = map(np.concatenate, [vec_ids, mut_ids, mut_sims])\n",
    "        affinity = sparse.csc_matrix(\n",
    "            (mut_sims, (vec_ids, mut_ids)), shape=(num, num), dtype=np.float32\n",
    "        )\n",
    "        return affinity\n",
    "\n",
    "\n",
    "def offline_diffusion_search(\n",
    "    queries,\n",
    "    train_embeddings,\n",
    "    labels,\n",
    "    truncation_size=1000,\n",
    "    kd=100,\n",
    "    cache_dir=\"./cache\",\n",
    "):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        queries: predicted embeddings\n",
    "        gallery: train embeddings\n",
    "        cache_dir: Directory to cache embeddings\n",
    "        truncation_size: Number of images in the truncated gallery\n",
    "        kd: top k results\n",
    "    \"\"\"\n",
    "    n_query = len(queries)\n",
    "    diffusion = Diffusion(\n",
    "        np.vstack([queries, train_embeddings]),\n",
    "        labels,\n",
    "        cache_dir=cache_dir,\n",
    "    )\n",
    "    offline = diffusion.get_offline_results(truncation_size, kd)\n",
    "    features = preprocessing.normalize(offline, norm=\"l2\", axis=1)\n",
    "    scores = features[:n_query] @ features[n_query:].T\n",
    "\n",
    "    ranks = np.argsort(-scores.toarray())[:, :kd]\n",
    "    return ranks"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc168e17",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "537eb97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicNet(nn.Module):\n",
    "    def __init__(self, output_features_size=512):\n",
    "        super().__init__()\n",
    "        self.output_features_size = output_features_size\n",
    "        self.transformer = nn.TransformerEncoder(\n",
    "            nn.TransformerEncoderLayer(\n",
    "                512, 8, dim_feedforward=2048, dropout=0.2, batch_first=True\n",
    "            ),\n",
    "            num_layers=3\n",
    "        )\n",
    "        self.avg_pooling = nn.AdaptiveAvgPool1d(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.permute((0, 2, 1))\n",
    "        x = self.transformer(x.float())\n",
    "        x = x.permute((0, 2, 1))\n",
    "        x = self.avg_pooling(x).squeeze()\n",
    "        return x\n",
    "    \n",
    "    \n",
    "def get_backbone(embed_dim):\n",
    "    net = BasicNet(embed_dim)\n",
    "    pooling = nn.Identity()\n",
    "    return net, pooling, embed_dim\n",
    "    \n",
    "    \n",
    "def str_to_bool(condition):\n",
    "    if isinstance(condition, str):\n",
    "        if condition.lower() == 'true':\n",
    "            condition = True\n",
    "        if condition.lower() == 'false':\n",
    "            condition = False\n",
    "    return condition\n",
    "    \n",
    "    \n",
    "class RetrievalNet(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        h_dim,\n",
    "        embed_dim=512,\n",
    "        norm_features=False,\n",
    "        without_fc=False,\n",
    "        with_autocast=False,\n",
    "        pooling=\"default\",\n",
    "        projection_normalization_layer=\"none\",\n",
    "        pretrained=True,\n",
    "        weights=None,\n",
    "    ):\n",
    "        super().__init__()\n",
    "\n",
    "        norm_features = str_to_bool(norm_features)\n",
    "        without_fc = str_to_bool(without_fc)\n",
    "        with_autocast = str_to_bool(with_autocast)\n",
    "\n",
    "        assert isinstance(without_fc, bool)\n",
    "        assert isinstance(norm_features, bool)\n",
    "        assert isinstance(with_autocast, bool)\n",
    "        self.norm_features = norm_features\n",
    "        self.without_fc = without_fc\n",
    "        self.with_autocast = with_autocast\n",
    "        if with_autocast:\n",
    "            print(\"Using mixed precision\")\n",
    "\n",
    "        self.backbone, default_pooling, out_features = get_backbone(h_dim)\n",
    "        if pooling == \"default\":\n",
    "            self.pooling = default_pooling\n",
    "        elif pooling == \"none\":\n",
    "            self.pooling = nn.Identity()\n",
    "        elif pooling == \"max\":\n",
    "            self.pooling = nn.AdaptiveMaxPool1d(output_size=1)\n",
    "        elif pooling == \"avg\":\n",
    "            self.pooling = nn.AdaptiveAvgPool1d(output_size=1)\n",
    "\n",
    "        if self.norm_features:\n",
    "            self.standardize = nn.LayerNorm(out_features, elementwise_affine=False)\n",
    "        else:\n",
    "            self.standardize = nn.Identity()\n",
    "\n",
    "        if not self.without_fc:\n",
    "            self.fc = create_projection_head(\n",
    "                out_features, embed_dim, projection_normalization_layer\n",
    "            )\n",
    "        else:\n",
    "            self.fc = nn.Identity()\n",
    "\n",
    "    def forward(self, X):\n",
    "        with torch.cuda.amp.autocast(enabled=self.with_autocast):\n",
    "            X = self.backbone(X)\n",
    "            X = self.pooling(X)\n",
    "            X = X.view(X.size(0), -1)\n",
    "            X = self.standardize(X)\n",
    "            X = self.fc(X)\n",
    "            X = F.normalize(X, p=2, dim=1)\n",
    "            return X"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05df8f58",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf9a1fe5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AudioEmbDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        data_dir,\n",
    "        csv_path,\n",
    "        crop_size=60,\n",
    "        mode=\"train\",\n",
    "    ):\n",
    "        self.get_fn = self._load_item\n",
    "\n",
    "        self.data_dir = data_dir\n",
    "        self.mode = mode\n",
    "        self.crop_size = crop_size\n",
    "        self.id_encoder = preprocessing.LabelEncoder()\n",
    "\n",
    "        self.meta_info = pd.read_csv(csv_path, sep=\"\\t\")\n",
    "\n",
    "        if mode != \"submission\":\n",
    "            self.meta_info[\"label\"] = self.id_encoder.fit_transform(self.meta_info[\"artistid\"])\n",
    "            self.classes_ = self.id_encoder.classes_\n",
    "            with open(\"classes.txt\", \"w\") as f:\n",
    "                f.write(\"\\n\".join(map(str, self.classes_)))\n",
    "            if \"stage\" in self.meta_info.columns:\n",
    "                self.meta_info = self.meta_info.loc[self.meta_info.stage == mode]\n",
    "            self.labels = self.meta_info.label.values\n",
    "        else:\n",
    "            self.labels = None\n",
    "\n",
    "        self.paths = self.meta_info.archive_features_path.values\n",
    "        self.track_ids = self.meta_info.trackid.values\n",
    "\n",
    "        if mode != \"submission\":\n",
    "            self.__get_instance_dict()\n",
    "    \n",
    "    def __get_instance_dict(self,):\n",
    "        self.instance_dict = {cl: [] for cl in set(self.labels)}\n",
    "        for idx, cl in enumerate(self.labels):\n",
    "            self.instance_dict[cl].append(idx)\n",
    "            \n",
    "    def __process_features(self, x):\n",
    "        x_len = x.shape[-1]\n",
    "        if x_len > self.crop_size:\n",
    "            start = np.random.randint(0, x_len - self.crop_size)\n",
    "            x = x[..., start : start + self.crop_size]\n",
    "        else:\n",
    "            if self.mode == \"train\":\n",
    "                i = np.random.randint(0, self.crop_size - x_len) if self.crop_size != x_len else 0\n",
    "            else:\n",
    "                i = (self.crop_size - x_len) // 2\n",
    "            pad_patern = (i, self.crop_size - x_len - i)\n",
    "            x = torch.nn.functional.pad(x, pad_patern, \"constant\").detach()\n",
    "        x = (x - x.mean()) / x.std()\n",
    "        return x\n",
    "\n",
    "\n",
    "    def _load_item(self, idx):\n",
    "        track_features_file_path = self.paths[idx]\n",
    "        track_features = torch.from_numpy(np.load(\n",
    "            os.path.join(self.data_dir, track_features_file_path)\n",
    "        ))\n",
    "        track_features = self.__process_features(track_features)\n",
    "\n",
    "        if self.labels is not None:\n",
    "            label = self.labels[idx]\n",
    "            label = torch.tensor([label])\n",
    "\n",
    "            out = {\n",
    "                \"features\": track_features,\n",
    "                \"label\": label,\n",
    "                \"track_id\": self.track_ids[idx],\n",
    "            }\n",
    "        else:\n",
    "            out = {\n",
    "                \"features\": track_features,\n",
    "                \"track_id\": self.track_ids[idx],\n",
    "            }\n",
    "            \n",
    "        return out\n",
    "\n",
    "    def __len__(self,):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.get_fn(idx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57b3a616",
   "metadata": {},
   "source": [
    "# Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "892e0fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data\n",
      "Validation set size: 16644\n",
      "Test set size: 41377\n",
      "\n",
      "Validation\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 33/33 [00:20<00:00,  1.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[offline] starting offline diffusion\n",
      "[offline] 1) prepare Laplacian and initial state\n",
      "[offline] 2) gallery-side diffusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[offline] diffusion: 100%|████████████████████████████████████████████████████████████████████████████████| 33288/33288 [00:26<00:00, 1275.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[offline] 3) merge offline results\n",
      "100 100 100.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████| 16644/16644 [00:01<00:00, 8347.74it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nDCG: 0.6848619341470749\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# INIT MODELS\n",
    "models = []\n",
    "for i in range(cfg.N_FOLDS):\n",
    "    weights_path = os.path.join(cfg.CHECKPOINT_PATH, f\"fold{i}.ckpt\")\n",
    "    model = RetrievalNet(\n",
    "        512,\n",
    "        512,\n",
    "        norm_features=True,\n",
    "        without_fc=True,\n",
    "        with_autocast=False\n",
    "    )\n",
    "    model.load_state_dict(torch.load(weights_path)[\"net_state\"])    \n",
    "    model.to(cfg.DEVICE)\n",
    "    model.eval()\n",
    "    models.append(model)\n",
    "\n",
    "# Load data\n",
    "meta_info = pd.read_csv(cfg.TRAIN_CSV_PATH, sep=\"\\t\")\n",
    "test_meta_info = pd.read_csv(cfg.TEST_CSV_PATH, sep=\"\\t\")\n",
    "validation_meta_info = meta_info[meta_info.stage == \"test\"].reset_index(drop=True)\n",
    "print(\"Loaded data\")\n",
    "print(\"Validation set size: {}\".format(len(validation_meta_info)))\n",
    "print(\"Test set size: {}\".format(len(test_meta_info)))\n",
    "print()\n",
    "\n",
    "# Validation\n",
    "print(\"Validation\")\n",
    "valid_ds = AudioEmbDataset(cfg.TRAIN_DATA_DIR, cfg.TRAIN_CSV_PATH, cfg.CROP_SIZE, \"test\")\n",
    "valid_loader = DataLoader(\n",
    "    valid_ds,\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "embeds = inference(models, valid_loader)\n",
    "submission = get_ranked_list_diffusion(embeds, 100)\n",
    "score = eval_submission(submission, validation_meta_info)\n",
    "\n",
    "print(f\"nDCG: {score}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98de022b",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "726e9a85",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submission\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████| 81/81 [00:47<00:00,  1.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[offline] starting offline diffusion\n",
      "[offline] 1) prepare Laplacian and initial state\n",
      "[offline] 2) gallery-side diffusion\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[offline] diffusion: 100%|████████████████████████████████████████████████████████████████████████████████| 82754/82754 [01:04<00:00, 1278.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[offline] 3) merge offline results\n",
      "99 100 99.99997583198395\n"
     ]
    }
   ],
   "source": [
    "print(\"Submission\")\n",
    "test_ds = AudioEmbDataset(cfg.TEST_DATA_DIR, cfg.TEST_CSV_PATH, cfg.CROP_SIZE, \"submission\")\n",
    "test_loader = DataLoader(\n",
    "    test_ds,\n",
    "    batch_size=cfg.BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    num_workers=4,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "embeds = inference(models, test_loader)\n",
    "submission = get_ranked_list_diffusion(embeds, 100)\n",
    "save_submission(submission, cfg.SUBMISSION_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0abf0897",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('yandex_cup')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "f940d30fd746d591d66ffc888c4cfe829d0710aef564c4062a6cdf6a03d4aec0"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
